#Sjekke om den er reprodusibelt
set.seed(123)
#Oppgave 2.4 , 2)
model2 <- glm(default ~ student, data = train,
family = "binomial")
summary(model2)
exp(coef(model2))
Student/ikkestudent <- data.frame(student = c("No","Yes" ))
student? <- data.frame(student = c("No","Yes" ))
stud_person <- data.frame(student = c("No","Yes" ))
StudPred <- predict(model2, newdata = stud_person, type = "response")
StudPred
#Oppgave 2.5
model3 <- glm(default ~ student + balance + income, data = train,
family = "binomial")
summary(model3)
exp(coef(model2))
exp(coef(model3))
#lager boxplot for student og balance
boxplot(balance ~ student, data = Default,
ylab = "balance", xlab = "student")
#Predikerer sannsynlighet for mislighold med lik gjeld
to_personer_stud <- data.frame(student = c("No","Yes" ))
#Predikerer sannsynlighet for mislighold med lik gjeld
to_personer_1500 <- data.frame(student = c("No","Yes" ), balance = c(1500))
to_personer_10000 <- data.frame(student = c("No","Yes" ), balance = c(10000))
Person_1500 <- predict(model3, newdata = to_personer_1500, type = "response")
#Predikerer sannsynlighet for mislighold med lik gjeld
to_personer_1500 <- data.frame(student = c("No","Yes" ), balance = c(1500), income = x)
#Predikerer sannsynlighet for mislighold med lik gjeld
to_personer_1500 <- data.frame(student = c("No","Yes" ), balance = c(1500), income = 10000))
#Predikerer sannsynlighet for mislighold med lik gjeld
to_personer_1500 <- data.frame(student = c("No","Yes" ), balance = c(1500), income = (10000))
Person_1500 <- predict(model3, newdata = to_personer_1500, type = "response")
Person_1500
library(caret)
# R-kode dersom vi vil velge k automatisk
set.seed(200)
trControl <- trainControl(method  = "cv", # 5-fold kryssvalidering
number  = 5)
# Tilpasser modellen
model4 <- train(default ~ balance + income + student,
data = train,
method = "knn",
trControl  = trControl,
metric     = "Accuracy")
# Hvilken k valgte kryssvalideringen?
k <- model4$finalModel$k
k
#Predict mislighold
to_kunder <- data.frame(balance = c(1000, 2000), income = 10000, student = c("Yes", "Yes"))
predict(model4, newdata = to_kunder)
#Oppgave 2.7
sann <- test$default  # Den sanne verdien av default i testdataene
klass_logreg <- ifelse(pred_logreg > 0.5, "Yes", "No")            # Klassifisering av kundene
pred_logreg <- predict(model3, newdata = test, type = "response") # Predikert sannsynlighet
klass_logreg <- ifelse(pred_logreg > 0.5, "Yes", "No")            # Klassifisering av kundene
logreg_tab <- table(sann, klass_logreg) # Kontigenstabell
logreg_tab
logreg_tab_norm <- logreg_tab %>%
prop.table %>%  # normaliser
round(3)        # rund av til 3 desimaler
logreg_tab_norm
logreg_tot <- sum(diag(logreg_tab_norm))               # Total andel korrekt klassfisering
logreg_tot
#Endrer klassifisering
klass_logreg <- ifelse(pred_logreg > 0.8, "Yes", "No")
klass_logreg <- ifelse(pred_logreg > 0.5, "Yes", "No")            # Klassifisering av kundene
logreg_tot
#Endrer klassifisering
klass_logreg1 <- ifelse(pred_logreg > 0.8, "Yes", "No")
logreg_tab1 <- table(sann, klass_logreg) # Kontigenstabell
logreg_tab1
logreg_tab1 <- table(sann, klass_logreg1) # Kontigenstabell
logreg_tab1
#Endrer klassifisering
klass_logreg1 <- ifelse(pred_logreg > 0.2, "Yes", "No")
logreg_tab1 <- table(sann, klass_logreg1) # Kontigenstabell
logreg_tab1
logreg_tab_norm1 <- logreg_tab %>%
prop.table %>%  # normaliser
round(3)        # rund av til 3 desimaler
logreg_tab_norm1 <- logreg_tab1 %>%
prop.table %>%  # normaliser
round(3)        # rund av til 3 desimaler
logreg_tab_norm1
# finne andel som er korrekt klassifisert
logreg_tot1 <- sum(diag(logreg_tab_norm1))               # Total andel korrekt klassfisering
logreg_tot1
pred_logreg2 <- predict(model4, newdata = test, type = "response") # Predikert sannsynlighet
pred_logreg2 <- predict(model4, newdata = test, type = "prob") # Predikert sannsynlighet
klass_logreg2 <- ifelse(pred_logreg2 > 0.5, "Yes", "No")            # Klassifisering av kundene
logreg_tab2 <- table(sann, klass_logreg2) # Kontigenstabell
#For model4
sann1 <- test$default  # Den sanne verdien av default i testdataene
logreg_tab2 <- table(sann1, klass_logreg2) # Kontigenstabell
klass_logreg2 <- ifelse(pred_logreg2 > 0.5, "Yes", "No")            # Klassifisering av kundene
pred_logreg2 <- predict(model4, newdata = test, type = "raw") # Predikert sannsynlighet
klass_logreg2 <- ifelse(pred_logreg2 > 0.5, "Yes", "No")            # Klassifisering av kundene
pred_logreg2 <- predict(model4, newdata = test, type = "prob") # Predikert sannsynlighet
klass_logreg2 <- ifelse(pred_logreg2 > 0.2, "Yes", "No")            # Klassifisering av kundene
logreg_tab2 <- table(sann1, klass_logreg2) # Kontigenstabell
logreg_tab
logreg_tab2
View(klass_logreg2)
View(model3)
logreg_tab2 <- table(sann1, pred_logreg2) # Kontigenstabell
View(klass_logreg2)
View(pred_logreg2)
pred_logreg2 <- predict(model4, newdata = test, type = "raw") # Predikert sannsynlighet
klass_logreg2 <- ifelse(pred_logreg2 = 1, "Yes", "No")            # Klassifisering av kundene
klass_logreg2 <- ifelse(pred_logreg2 > 0,5, "Yes", "No")            # Klassifisering av kundene
klass_logreg2 <- ifelse(pred_logreg2 > 0.5 , "Yes", "No")            # Klassifisering av kundene
klass_logreg2 <- ifelse(pred_logreg2 < 0.5 , "Yes", "No")            # Klassifisering av kundene
logreg_tab2 <- table(sann1, klass_logreg2) # Kontigenstabell
logreg_tab2
klass_logreg2 <- ifelse(pred_logreg2 = 0 , "Yes", "No")            # Klassifisering av kundene
pred_logreg2 <- predict(model4, newdata = test, type = "pred") # Predikert sannsynlighet
pred_logreg2 <- predict(model4, newdata = test, type = "prob") # Predikert sannsynlighet
klass_logreg2 <- ifelse(pred_logreg2 = 0 , "Yes", "No")            # Klassifisering av kundene
pred_logreg2 <- predict(model4, newdata = test, type = "raw") # Predikert sannsynlighet
pred_logreg2 <- predict(model4, newdata = to_kunder) # Predikert sannsynlighet
klass_logreg2 <- ifelse(pred_logreg2 > 0,5 , "Yes", "No")            # Klassifisering av kundene
klass_logreg2 <- ifelse(pred_logreg2 > 0.5 , "Yes", "No")            # Klassifisering av kundene
klass_logreg2 <- ifelse(pred_logreg2 = 0 , "Yes", "No")            # Klassifisering av kundene
pred_logreg2 <- predict(model4, newdata = to_kunder) # Predikert sannsynlighet
pred_logreg2 <- predict(model4, newdata = test) # Predikert sannsynlighet
klass_logreg2 <- ifelse(pred_logreg2 = 0 , "Yes", "No")            # Klassifisering av kundene
qf(1 - 0.05/2, df1 = 268 - 1, df2 = 195 - 1)
qf(1 - 0.05, df1 = 268 - 1, df2 = 195 - 1)
qf(1 - 0.05/2, df1 = 195 - 1, df2 = 268 - 1)
qf(1 - 0.05/2, df1 = 268 - 1, df2 = 195 - 1)
qf(1 - 0.01/2, df1 = 50 - 1, df2 = 50 - 1)
qt(0.01, df = 50 - 1)
qt(0.01, df = 49 - 1)
qt(0.01/2, df = 50 - 1)
qf(1 - 0.05/2, df1 = 15 - 1, df2 = 15 - 1)
qt(0.05/2, df = 18 - 1)
qt(0.05/2)
qt(0.05/2, df = 18 - 1)
load("~/Desktop/met4_h22_heks_forberedelse/met4_h22.RData")
install.packages("languageserver")
install.packages("VSCode-R-Debugger")
transform_metadata_to_df <- function(stations_metadata) {
df <- stations_metadata[[1]] %>%
map(as_tibble) %>%
list_rbind() %>%
mutate(latestData = map_chr(latestData, 1, .default = NA_character_)) %>%
mutate(latestData = as_datetime(latestData, tz = "Europe/Berlin")) %>%
mutate(latestData = with_tz(latestData, "UTC")) %>%
mutate(location = map(location, unlist)) %>%
mutate(
lat = map_dbl(location, "latLon.lat"),
lon = map_dbl(location, "latLon.lon")
) %>%
select(-location)
return(df)
}
to_iso8601 <-
function(datetime_var, offset_in_days){
adjusted_datetime <- datetime_var + days(offset_in_days)
iso8601_string <- format(with_tz(adjusted_datetime, "UTC"),
"%Y-%m-%dT%H:%M:%SZ")
return(iso8601_string)
}
transform_volumes <- function(data) {
volumes <- data$trafficData$volume$byHour$edges
df <- purrr::map_dfr(volumes, ~{
tibble::tibble(
from = .x$node$from,
to = .x$node$to,
volume = .x$node$total$volumeNumbers$volume
)
})
return(df)
}
test_stations_metadata_colnames <-
function(df) {
expected_colnames <- c("id", "name", "latestData", "lat", "lon")
if (all(colnames(df) == expected_colnames) == TRUE) {
print("PASS: Data has the correct columns")
} else{
print("FAIL: Columns do not match the correct specification")
}
}
test_stations_metadata_nrows <-
function(df) {
min_expected_rows <- 5000
max_expected_rows <- 10000
if (nrow(df) > min_expected_rows & nrow(df) < max_expected_rows) {
print("PASS: Data has a reasonable number of rows")
} else if (nrow(df) <= min_expected_rows) {
print("FAIL: Data has suspiciously few rows")
} else {
print("FAIL: Data has suspiciously many rows")
}
}
test_stations_metadata_coltypes <-
function(df) {
expected_coltypes <-
c("character", "character", "double", "double", "double")
if (all(df %>%
map_chr( ~ typeof(.)) == expected_coltypes) == TRUE) {
print("PASS: All cols have the correct specifications")
} else{
print("FAIL: Columns do not have the correct specification")
}
}
test_stations_metadata_nmissing <-
function(df) {
max_miss_vals <- 200
if (df %>% map_int( ~ sum(is.na((.)))) %>% sum(.) < max_miss_vals) {
print("PASS: Amount of missing values is reasonable")
} else {
print("FAIL: Too many missing values in data set")
}
}
test_stations_metadata_latestdata_timezone <-
function(df) {
if (attr(df$latestData,"tzone")=="UTC") {
print("PASS: latestData has UTC-time zone")
} else {
print("FAIL: latestData does not have expected UTC-time zone")
}
}
# This funciton combines all the other functions
test_stations_metadata <-
function(df){
test_stations_metadata_colnames(df)
test_stations_metadata_coltypes(df)
test_stations_metadata_nmissing(df)
test_stations_metadata_nrows(df)
test_stations_metadata_latestdata_timezone(df)
}
# Load function for posting GQL-queries and retrieving data:
source("functions/GQL_function.r")
library(httr)
library(jsonlite)
library(ggplot2)
library(DescTools)
library(tidyverse)
library(magrittr)
library(rlang)
library(lubridate)
library(anytime)
library(readr)
library(yaml)
# Load function for posting GQL-queries and retrieving data:
source("functions/GQL_function.r")
setwd("/Users/99havwik/Desktop/BAN400/Assignments")
# Load function for posting GQL-queries and retrieving data:
source("functions/GQL_function.r")
setwd("~/Documents/GitHub/iterations-Haavardwikse")
# Load function for posting GQL-queries and retrieving data:
source("functions/GQL_function.r")
configs <-
read_yaml("vegvesen_configs.yml")
gql_metadata_qry <- read_file("gql-queries/station_metadata.gql")
stations_metadata <-
GQL(
query=gql_metadata_qry,
.url = configs$vegvesen_url
)
source("functions/data_transformations.r")
stations_metadata_df <-
stations_metadata %>%
transform_metadata_to_df(.)
#### 3: Testing metadata
source("functions/data_tests.r")
test_stations_metadata(stations_metadata_df)
source("gql-queries/vol_qry.r")
setwd("~/Documents/GitHub/iterations-Haavardwikse")
source("gql-queries/vol_qry.r")
setwd("/Users/99havwik/Desktop/BAN400/Assignments")
setwd("~/Documents/GitHub/iterations-Haavardwikse")
source("gql-queries/vol_qry.r")
source("gql-queries/vol_qry.r")
source("gql-queries/vol_qry.r")
setwd("~/Documents/GitHub/iterations-Haavardwikse")
source("gql-queries/vol_qry.r")
source("functions/data_transformations.r")
stations_metadata_df <-
stations_metadata %>%
transform_metadata_to_df(.)
#### 3: Testing metadata
source("functions/data_tests.r")
test_stations_metadata(stations_metadata_df)
source("gql-queries/vol_qry.r")
# Vol_qry function
vol_qry <- function(id, from, to) {
query <- glue::glue(
'{
trafficData(trafficRegistrationPointId: "[id]") {
volume {
byHour(from: "[from]", to: "[to]") {
edges {
node {
from
to
total {
volumeNumbers {
volume
}
}
}
}
}
}
}
}',
.open = "[",
.close = "]"
)
return(query)
}
source("gql-queries/vol_qry.r")
vol_qry <- function(id, from, to) {
query <- glue::glue(
'{
trafficData(trafficRegistrationPointId: "[id]") {
volume {
byHour(from: "[from]", to: "[to]") {
edges {
node {
from
to
total {
volumeNumbers {
volume
}
}
}
}
}
}
}
}',
.open = "[",
.close = "]"
)
return(query)
}
source("gql-queries/vol_qry.r")
source("gql-queries/vol_qry.r")
source("gql-queries/vol_qry.r")
source("gql-queries/vol_qry.r")
stations_metadata_df %>%
filter(latestData > Sys.Date() - days(7)) %>%
sample_n(1) %$%
vol_qry(
id = id,
from = to_iso8601(latestData, -4),
to = to_iso8601(latestData, 0)
) %>%
GQL(., .url = configs$vegvesen_url) %>%
transform_volumes() %>%
ggplot(aes(x=from, y=volume)) +
geom_line() +
theme_classic()
### 6: Changing the plot
stations_metadata_df %>%
filter(latestData > Sys.Date() - days(7)) %>%
# Randomly samples one station from the filtered data and stores it in `sampled_station`
sample_n(1) -> sampled_station
# Extracts the name of the sampled station and stores it in `station_name`
station_name <- sampled_station$name
sampled_station %$%
vol_qry(
id = id,
from = to_iso8601(latestData, -4),
to = to_iso8601(latestData, 0)
) %>%
GQL(., .url = configs$vegvesen_url) %>%
transform_volumes() %>%
# Converts the `from` column to POSIXct format for proper date-time plotting
mutate(from = as.POSIXct(from, format="%Y-%m-%dT%H:%M:%S")) %>%
ggplot(aes(x=from, y=volume, group = 1)) +
geom_line(aes(color = station_name)) +
theme_classic() +
# Labels the color legend as 'Traffic Station'
labs(color = 'Traffic Station') +
# Formats the x-axis labels as date-time
scale_x_datetime(labels = scales::date_format("%Y-%m-%d %H:%M"))
source("gql-queries/vol_qry.r")
stations_metadata_df %>%
filter(latestData > Sys.Date() - days(7)) %>%
sample_n(1) %$%
vol_qry(
id = id,
from = to_iso8601(latestData, -4),
to = to_iso8601(latestData, 0)
) %>%
GQL(., .url = configs$vegvesen_url) %>%
transform_volumes() %>%
ggplot(aes(x=from, y=volume)) +
geom_line() +
theme_classic()
stations_metadata_df %>%
filter(latestData > Sys.Date() - days(7)) %>%
sample_n(1) -> sampled_station
station_name <- sampled_station$name
sampled_station %$%
vol_qry(
id = id,
from = to_iso8601(latestData, -4),
to = to_iso8601(latestData, 0)
) %>%
GQL(., .url = configs$vegvesen_url) %>%
transform_volumes() %>%
mutate(from = as.POSIXct(from, format="%Y-%m-%dT%H:%M:%S")) %>%
ggplot(aes(x=from, y=volume, group = 1)) +
geom_smooth(aes(color = station_name), se = FALSE) +  # Smoothen the line
geom_point(aes(color = station_name), alpha = 0.5) +  # Highlight data points
labs(
title = "Traffic Volume Over Time",
subtitle = paste("Data from:", station_name),
x = "Date & Time",
y = "Volume",
color = 'Traffic Station'
) +
scale_x_datetime(labels = scales::date_format("%Y-%m-%d %H:%M")) +
theme_minimal() +  # Use a minimal theme for a clean look
theme(
plot.title = element_text(hjust = 0.5, size = 20),      # Center title and adjust size
plot.subtitle = element_text(hjust = 0.5, size = 15)    # Center subtitle and adjust size
)
sampled_station %$%
vol_qry(
id = id,
from = to_iso8601(latestData, -4),
to = to_iso8601(latestData, 0)
) %>%
GQL(., .url = configs$vegvesen_url) %>%
transform_volumes() %>%
mutate(from = as.POSIXct(from, format="%Y-%m-%dT%H:%M:%S")) %>%
ggplot(aes(x=from, y=volume, group = 1)) +  # Smoothen the line
geom_point(aes(color = station_name), alpha = 0.5) +  # Highlight data points
labs(
title = "Traffic Volume Over Time",
subtitle = paste("Data from:", station_name),
x = "Date & Time",
y = "Volume",
color = 'Traffic Station'
) +
scale_x_datetime(labels = scales::date_format("%Y-%m-%d %H:%M")) +
theme_minimal() +  # Use a minimal theme for a clean look
theme(
plot.title = element_text(hjust = 0.5, size = 20),      # Center title and adjust size
plot.subtitle = element_text(hjust = 0.5, size = 15)    # Center subtitle and adjust size
)
stations_metadata_df %>%
filter(latestData > Sys.Date() - days(7)) %>%
sample_n(1) %$%
vol_qry(
id = id,
from = to_iso8601(latestData, -4),
to = to_iso8601(latestData, 0)
) %>%
GQL(., .url = configs$vegvesen_url) %>%
transform_volumes() %>%
ggplot(aes(x=from, y=volume)) +
geom_line() +
theme_classic()
stations_metadata_df %>%
filter(latestData > Sys.Date() - days(7)) %>%
sample_n(1) -> sampled_station
station_name <- sampled_station$name
sampled_station %$%
vol_qry(
id = id,
from = to_iso8601(latestData, -4),
to = to_iso8601(latestData, 0)
) %>%
GQL(., .url = configs$vegvesen_url) %>%
transform_volumes() %>%
mutate(from = as.POSIXct(from, format="%Y-%m-%dT%H:%M:%S")) %>%
ggplot(aes(x=from, y=volume, group = 1)) +  # Smoothen the line
geom_point(aes(color = station_name), alpha = 0.5) +  # Highlight data points
labs(
title = "Traffic Volume Over Time",
subtitle = paste("Data from:", station_name),
x = "Date & Time",
y = "Volume",
color = 'Traffic Station'
) +
scale_x_datetime(labels = scales::date_format("%Y-%m-%d %H:%M")) +
theme_minimal() +  # Use a minimal theme for a clean look
theme(
plot.title = element_text(hjust = 0.5, size = 20),      # Center title and adjust size
plot.subtitle = element_text(hjust = 0.5, size = 15)    # Center subtitle and adjust size
)
#improving plot
stations_metadata_df %>%
filter(latestData > Sys.Date() - days(7)) %>%
sample_n(1) -> sampled_station
station_name <- sampled_station$name
sampled_station %$%
vol_qry(
id = id,
from = to_iso8601(latestData, -4),
to = to_iso8601(latestData, 0)
) %>%
GQL(., .url = configs$vegvesen_url) %>%
transform_volumes() %>%
mutate(from = as.POSIXct(from, format="%Y-%m-%dT%H:%M:%S")) %>%
ggplot(aes(x=from, y=volume, group = 1)) +
geom_line(aes(color = station_name)) +
geom_point(aes(color = station_name), size = 2, alpha = 0.7) +  # Adjust point size and transparency
scale_color_brewer(palette = "Set2") +  # Using a Brewer palette for pleasant colors
theme_minimal() +  # Use a minimal theme for a clean look
labs(color = 'Traffic Station') +
scale_x_datetime(labels = scales::date_format("%Y-%m-%d %H:%M"))
